{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136702c1",
   "metadata": {},
   "source": [
    "## See revisions.docx for all changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40d43e",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76425d52",
   "metadata": {},
   "source": [
    "## FAQ 1. Sentiment Thresholds Should Not Be Arbitrary\n",
    "\n",
    "Fix Made:\n",
    "added markdown:\n",
    "This project uses TextBlob to assign sentiment labels based on polarity scores. A threshold of ±0.1 was used to label Neutral sentiment:\n",
    "- Polarity > 0.1 → Positive  \n",
    "- Polarity < –0.1 → Negative  \n",
    "- Otherwise → Neutral\n",
    "This threshold was chosen to avoid misclassifying slightly opinionated or ambiguous text as strongly positive or negative. A wider neutral range (−0.1 to 0.1) helps reduce false positives and makes the model more conservative, which is appropriate in a professional email context where extreme sentiment is relatively rare. While higher thresholds (e.g., ±0.2) were considered, they resulted in too many emails being labeled Neutral, losing useful signal for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c970ca",
   "metadata": {},
   "source": [
    "Task 1: Sentiment Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54081687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/raw/test.csv')\n",
    "print(df.isna().mean().sort_values(ascending=False))\n",
    "df = df.fillna('') \n",
    "\n",
    "# Combine subject and body into a single text column\n",
    "df['text'] = df.apply(lambda row: row['body'] if row['body'].strip() else row['Subject'], axis=1)\n",
    "df = df[df['text'].str.strip() != '']\n",
    "\n",
    "# Label sentiment with TextBlob (original)\n",
    "def classify_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return 'Positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "tqdm.pandas()\n",
    "df['Sentiment'] = df['text'].progress_apply(classify_sentiment)\n",
    "\n",
    "# --------- ADDITION: Transformer-based Sentiment Model ---------\n",
    "# Load HuggingFace sentiment pipeline (Roberta model)\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Classify using the transformer model\n",
    "def classify_with_roberta(text):\n",
    "    try:\n",
    "        result = sentiment_pipe(text[:512])[0]['label']\n",
    "        # Map labels to match your format\n",
    "        if result == 'LABEL_2':\n",
    "            return 'Positive'\n",
    "        elif result == 'LABEL_0':\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    except:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment_Roberta'] = df['text'].progress_apply(classify_with_roberta)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Save labeled data\n",
    "os.makedirs('./data/processed', exist_ok=True)\n",
    "df.to_csv('./data/processed/labeled_messages.csv', index=False)\n",
    "\n",
    "# Print value counts of both models\n",
    "print(\"TextBlob Sentiment Distribution:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\nRoberta Sentiment Distribution:\")\n",
    "print(df['Sentiment_Roberta'].value_counts())\n",
    "\n",
    "# Filter where the two sentiment labels disagree\n",
    "disagreements = df[df['Sentiment'] != df['Sentiment_Roberta']]\n",
    "# Display 7 sample disagreements\n",
    "sample_disagreements = disagreements.sample(7, random_state=42)\n",
    "# Print text and both sentiment values\n",
    "for idx, row in sample_disagreements.iterrows():\n",
    "    print(f\"--- Sample {idx} ---\")\n",
    "    print(\"Text:\")\n",
    "    print(row['text'])\n",
    "    print(\"\\nTextBlob Sentiment:\", row['Sentiment'])\n",
    "    print(\"Roberta Sentiment:\", row['Sentiment_Roberta'])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1af8e",
   "metadata": {},
   "source": [
    "## FAQ 2. Don’t Rely on One Sentiment Tool (Without Validation)\n",
    "\n",
    "Fix Made:\n",
    "Initially, I used only TextBlob for sentiment analysis. I added a second column of predictions using a transformer-based Roberta model (cardiffnlp/twitter-roberta-base-sentiment). I then compared the two outputs side by side.\n",
    "\n",
    "The predictions of the models side by side:\n",
    "TextBlob Sentiment Distribution:\n",
    "Sentiment\n",
    "Neutral     1053\n",
    "Positive     974\n",
    "Negative     164\n",
    "Name: count, dtype: int64\n",
    "\n",
    "Roberta Sentiment Distribution:\n",
    "Sentiment_Roberta\n",
    "Neutral     1485\n",
    "Positive     558\n",
    "Negative     148\n",
    "Name: count, dtype: int64\n",
    "\n",
    "This discrepancy suggests that TextBlob tends to assign more messages as positive, while Roberta is more conservative, possibly due to domain differences.\n",
    "\n",
    "TextBlob tended to label more messages at a more sensitive scale, especially on the positive side, while Roberta more frequently predicted Neutral. After manually reviewing the messages with differing predictions, I found that Roberta’s outputs were generally more aligned with the actual tone of the emails.\n",
    "\n",
    "Conclusion:\n",
    "I added this second model to validate the accuracy of the initial tool. Roberta’s predictions were found to be more appropriate for our formal business email dataset, and aligns with FAQ guidance to avoid relying on a single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fcad8",
   "metadata": {},
   "source": [
    "Task 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083312ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "# Paths\n",
    "RAW_DIR = Path(\"./data/raw\")\n",
    "PROC_DIR = Path(\"./data/processed\")\n",
    "VIZ_DIR = Path(\"./visualization\")\n",
    "VIZ_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#load saved data\n",
    "df = pd.read_csv(PROC_DIR / \"labeled_messages.csv\")\n",
    "print(\"Rows:\", len(df))\n",
    "df.head()\n",
    "\n",
    "#date cleaning & parsing\n",
    "def clean_date(x):\n",
    "    \"\"\"Convert '########' or empty strings to NaT, else parse M/D/YYYY.\"\"\"\n",
    "    if isinstance(x, str) and x.strip().startswith(\"#\"):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "df[\"date_parsed\"] = df[\"date\"].apply(clean_date)\n",
    "#save\n",
    "df.to_csv(PROC_DIR / \"labeled_messages.csv\", index=False)\n",
    "\n",
    "print(\"Date parsing success rate:\", df[\"date_parsed\"].notna().mean())\n",
    "missing_summary = df.isna().mean().rename(\"missing_ratio\").to_frame()\n",
    "display(missing_summary)\n",
    "\n",
    "#sentiment distribution\n",
    "sns.countplot(x=\"Sentiment\", data=df, order=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"sentiment_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "#time series sentiment trend\n",
    "df_time = (\n",
    "    df.dropna(subset=[\"date_parsed\"])\n",
    "      .assign(month=lambda d: d[\"date_parsed\"].dt.to_period(\"M\"))\n",
    "      .groupby([\"month\", \"Sentiment\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "df_time.plot(kind=\"bar\", stacked=False)\n",
    "plt.title(\"Monthly Message Count by Sentiment\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"monthly_sentiment_counts.png\")\n",
    "plt.close()\n",
    "\n",
    "#top senders by message volume\n",
    "top_senders = (\n",
    "    df.groupby(\"from\")[\"text\"].count().sort_values(ascending=False).head(10)\n",
    ")\n",
    "sns.barplot(y=top_senders.index, x=top_senders.values, orient=\"h\")\n",
    "plt.title(\"Top 10 Employees by # Messages\")\n",
    "plt.xlabel(\"Message Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / \"top_senders.png\")\n",
    "plt.close()\n",
    "\n",
    "#data summary:\n",
    "# Shape and column names\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\\n\", df.columns)\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "\n",
    "# Missing value ratio\n",
    "missing_summary = df.isna().mean().rename(\"missing_ratio\").to_frame()\n",
    "print(\"\\nMissing ratio per column:\\n\", missing_summary)\n",
    "\n",
    "# Distribution of sentiment values\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\nSentiment distribution (normalized):\\n\", df['Sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Drop NA dates and convert to month\n",
    "df_time = (\n",
    "    df.dropna(subset=[\"date_parsed\"])\n",
    "      .assign(month=lambda d: d[\"date_parsed\"].dt.to_period(\"M\"))\n",
    "      .groupby([\"month\", \"Sentiment\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "print(\"\\nMonthly sentiment trend:\\n\", df_time.tail(12))  # last 12 months, for brevity\n",
    "\n",
    "# Who sends the most messages?\n",
    "top_senders = df['from'].value_counts().head(10)\n",
    "print(\"\\nTop 10 employees by message count:\\n\", top_senders)\n",
    "\n",
    "# Check if any senders only send negative messages\n",
    "only_negative = df[df[\"Sentiment\"] == \"Negative\"][\"from\"].value_counts()\n",
    "print(\"\\nEmployees with most negative messages:\\n\", only_negative.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592c821",
   "metadata": {},
   "source": [
    "\n",
    "## FAQ 3. Charts Without Interpretation Are Not Insightful\n",
    "\n",
    "Fix Made: \n",
    "each chart interpreted with markdown:\n",
    "# Revision\n",
    "Sentiment was distributed heavily toward Positive and Neutral, with approximately 1,000 emails each. In contrast, only about 150 emails were labeled Negative, indicating that negative sentiment is relatively rare in the dataset. This imbalance suggests that most employee communications maintain a neutral or constructive tone, which is typical in professional environments.\n",
    "# Revision\n",
    "Sentiment was distributed fairly evenly across most months, suggesting a consistent tone in employee communications throughout the year. However, there was a slight dip in sentiment during January 2010, with a noticeable increase in negative emails — approximately 14 messages labeled as Negative. This could indicate a temporary period of dissatisfaction or tension.\n",
    "# Revision\n",
    "The top 10 most active senders include two individuals with approximately 175 messages each, six with around 225 messages, one with 250, and one with nearly 275 messages. This distribution indicates that one small subset of individuals is responsible for a slightly larger portion of total communication, and another is responsible for a slightly smaller portion. The two high-volume senders may hold key roles in the organization—such as management, team leads, or coordinators—and could significantly influence the overall tone and sentiment in the dataset. Their communication patterns are critical to monitor, as changes in their sentiment or volume might reflect broader shifts within their teams or departments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2747f8",
   "metadata": {},
   "source": [
    "## FAQ 4. Avoid Inventing Metrics Without Rationale\n",
    "\n",
    "No changes needed: \n",
    "Did not create custom metrics. The sentiment score is a simple sum of labeled values. \n",
    "The monthly sentiment score directly aggregates labeled values with no weighting or scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7b3ad",
   "metadata": {},
   "source": [
    "Task 3: Employee Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Labeled Data\n",
    "df = pd.read_csv(\"./data/processed/labeled_messages.csv\")\n",
    "def clean_date(x):\n",
    "    \"\"\"Convert '########' or empty strings to NaT, else parse M/D/YYYY.\"\"\"\n",
    "    if isinstance(x, str) and x.strip().startswith(\"#\"):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime(x, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "df[\"date_parsed\"] = df[\"date\"].apply(clean_date)\n",
    "#save parsed date column\n",
    "df.to_csv(\"./data/processed/labeled_messages.csv\", index=False)\n",
    "df = df.dropna(subset=[\"date_parsed\"])  # Drop rows without date\n",
    "\n",
    "#map scores\n",
    "sentiment_map = {\"Positive\": 1, \"Negative\": -1, \"Neutral\": 0}\n",
    "df[\"Sentiment_Score\"] = df[\"Sentiment\"].map(sentiment_map)\n",
    "\n",
    "df[\"YearMonth\"] = df[\"date_parsed\"].dt.to_period(\"M\")\n",
    "\n",
    "#combine monthly scores\n",
    "monthly_scores = (\n",
    "    df.groupby([\"from\", \"YearMonth\"])[\"Sentiment_Score\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"from\": \"Employee\", \"YearMonth\": \"Month\", \"Sentiment_Score\": \"Score\"})\n",
    ")\n",
    "\n",
    "#save data\n",
    "print(monthly_scores.head())\n",
    "monthly_scores.to_csv(\"./data/processed/monthly_sentiment_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e85517",
   "metadata": {},
   "source": [
    "Task 4: Employee ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Monthly Scores\n",
    "df = pd.read_csv(\"./data/processed/monthly_sentiment_scores.csv\")\n",
    "df[\"Month\"] = pd.PeriodIndex(df[\"Month\"], freq=\"M\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Define Ranking Logic\n",
    "def get_rankings(group):\n",
    "    top = (\n",
    "        group.sort_values(by=[\"Score\", \"Employee\"], ascending=[False, True])\n",
    "             .head(3)\n",
    "             .assign(Rank_Type=\"Top Positive\")\n",
    "    )\n",
    "    bottom = (\n",
    "        group.sort_values(by=[\"Score\", \"Employee\"], ascending=[True, True])\n",
    "             .head(3)\n",
    "             .assign(Rank_Type=\"Top Negative\")\n",
    "    )\n",
    "    return pd.concat([top, bottom])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Apply Ranking Per Month\n",
    "rankings = df.groupby(\"Month\", group_keys=False).apply(get_rankings).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(rankings.head(10))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save Rankings to File\n",
    "rankings.to_csv(\"./data/processed/monthly_employee_rankings.csv\", index=False)\n",
    "\n",
    "#get overall top positive and negative employees\n",
    "rankings = pd.read_csv(\"./data/processed/monthly_employee_rankings.csv\")\n",
    "\n",
    "# Assign +1 for Top Positive, ‑1 for Top Negative\n",
    "rankings[\"point\"] = rankings[\"Rank_Type\"].map({\"Top Positive\": 1, \"Top Negative\": -1})\n",
    "\n",
    "# Aggregate points across all months\n",
    "overall_scores = (\n",
    "    rankings.groupby(\"Employee\")[\"point\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"point\": \"Overall_Score\"})\n",
    ")\n",
    "\n",
    "# Sort for global Top Positive (highest) and Top Negative (lowest)\n",
    "top_global_positive = (\n",
    "    overall_scores.sort_values(by=[\"Overall_Score\", \"Employee\"], ascending=[False, True])\n",
    "    .head(3)\n",
    "    .assign(Global_Rank=\"Top Positive\")\n",
    ")\n",
    "\n",
    "top_global_negative = (\n",
    "    overall_scores.sort_values(by=[\"Overall_Score\", \"Employee\"], ascending=[True, True])\n",
    "    .head(3)\n",
    "    .assign(Global_Rank=\"Top Negative\")\n",
    ")\n",
    "\n",
    "global_top3 = pd.concat([top_global_positive, top_global_negative])\n",
    "print(global_top3)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save Global Rankings\n",
    "global_top3.to_csv(\"./data/processed/global_top3_employees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced81a7",
   "metadata": {},
   "source": [
    "Task 5: Flight Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b777d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "df = pd.read_csv(\"./data/processed/labeled_messages.csv\")\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date_parsed\"], errors=\"coerce\")\n",
    "\n",
    "# Keep only negative messages with a valid date\n",
    "df_neg = df[(df[\"Sentiment\"] == \"Negative\") & (df[\"date_parsed\"].notna())]\n",
    "df_neg = df_neg.sort_values([\"from\", \"date_parsed\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Identify Rolling 30-day Negative Message Clusters\n",
    "def flag_risk(group):\n",
    "    risk_dates = []\n",
    "    dates = group[\"date_parsed\"].tolist()\n",
    "    for i in range(len(dates)):\n",
    "        count = 1\n",
    "        start = dates[i]\n",
    "        for j in range(i+1, len(dates)):\n",
    "            if (dates[j] - start).days <= 30:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        if count >= 4:\n",
    "            risk_dates.append(start)\n",
    "    return pd.Series({\"At_Risk\": len(risk_dates) > 0})\n",
    "\n",
    "risk_flags = df_neg.groupby(\"from\").apply(flag_risk).reset_index()\n",
    "risk_flags = risk_flags.rename(columns={\"from\": \"Employee\"})\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Output & Save\n",
    "print(risk_flags[risk_flags[\"At_Risk\"] == True])\n",
    "risk_flags.to_csv(\"./data/processed/flight_risk_employees.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2cd7f",
   "metadata": {},
   "source": [
    "## FAQ 6. Thoughtful Feature Selection in Modeling\n",
    "\n",
    "No changes needed:\n",
    "Features chosen were message count, average length, and average word count, based on proven logical relevance to emotional tone and frequency of communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e758d",
   "metadata": {},
   "source": [
    "Task 6: Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# load and preprocess data\n",
    "df = pd.read_csv(\"./data/processed/labeled_messages.csv\")\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date_parsed\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date_parsed\"])\n",
    "\n",
    "# Sentiment to numeric\n",
    "sentiment_map = {\"Positive\": 1, \"Negative\": -1, \"Neutral\": 0}\n",
    "df[\"Sentiment_Score\"] = df[\"Sentiment\"].map(sentiment_map)\n",
    "\n",
    "# Message features\n",
    "df[\"char_count\"] = df[\"text\"].astype(str).apply(len)\n",
    "df[\"word_count\"] = df[\"text\"].astype(str).apply(lambda x: len(x.split()))\n",
    "df[\"Month\"] = df[\"date_parsed\"].dt.to_period(\"M\")\n",
    "\n",
    "# Group & Feature Engineering\n",
    "monthly_df = df.groupby([\"from\", \"Month\"]).agg({\n",
    "    \"text\": \"count\",\n",
    "    \"char_count\": \"mean\",\n",
    "    \"word_count\": \"mean\",\n",
    "    \"Sentiment_Score\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "monthly_df = monthly_df.rename(columns={\n",
    "    \"from\": \"Employee\",\n",
    "    \"text\": \"msg_count\",\n",
    "    \"char_count\": \"avg_msg_length\",\n",
    "    \"word_count\": \"avg_word_count\",\n",
    "    \"Sentiment_Score\": \"sentiment_score\"\n",
    "})\n",
    "\n",
    "# Train/Test Split\n",
    "features = [\"msg_count\", \"avg_msg_length\", \"avg_word_count\"]\n",
    "X = monthly_df[features]\n",
    "y = monthly_df[\"sentiment_score\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Evaluation\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120e0ca",
   "metadata": {},
   "source": [
    "## FAQ 7. Don’t Just Print R² and MSE — Interpret Them\n",
    "\n",
    "Fix Made: \n",
    "added markdown:\n",
    "The R² score of 0.42 indicates that approximately 42% of the variance in the target variable (e.g., monthly sentiment score) is explained by the model's input features. While this reflects a moderate level of explanatory power, over half of the variability remains unexplained, suggesting that either the current features lack sufficient predictive strength or that sentiment patterns are influenced by external factors not captured in the dataset.\n",
    "The RMSE of 1.99 means that on average, the model's predictions deviate from the actual sentiment score by nearly 2 units, a moderate amount considering that it ranges from -1 to 12 throughout the dataset. This suggests that while the model can recognize some broad trends, it struggles to make precise predictions at the individual message or monthly level, and needs further refinement through feature selection.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
